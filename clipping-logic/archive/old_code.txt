
def resize_image_to_same_height(image_data, target_height=300):
    # Try to detect if the image is an SVG by looking at the first few bytes
    if image_data[:5].lower() == b'<?xml' or b'<svg' in image_data[:50].lower():
        # Parse the SVG data
        tree = ET.ElementTree(ET.fromstring(image_data))
        svg = tree.getroot()
        # Check for viewBox attribute
        viewBox = svg.get('viewBox')
        if viewBox:
            _, _, vb_width, vb_height = map(float, viewBox.split())
        else:
            vb_width = vb_height = None

        width = svg.get('width')
        height = svg.get('height')
        
        if width and height:
            original_width = float(width.strip('px'))
            original_height = float(height.strip('px'))
        elif vb_width and vb_height:
            original_width, original_height = vb_width, vb_height
        else:
            raise ValueError("SVG does not have sufficient information for resizing (no width/height or viewBox)")

        # Calculate new dimensions
        scale_ratio = target_height / original_height
        new_width = int(original_width * scale_ratio)

        # Set new width and height
        svg.set('width', f"{new_width}px")
        svg.set('height', f"{target_height}px")

        # Convert tree back to bytes
        output = io.BytesIO()
        tree.write(output, encoding='utf-8', xml_declaration=True)
        return output.getvalue()
    else:
        # Handle raster images
        img = Image.open(io.BytesIO(image_data))
        original_width, original_height = img.size
        scale_ratio = target_height / original_height
        new_width = int(original_width * scale_ratio)
        resized_img = img.resize((new_width, target_height), Image.Resampling.LANCZOS)
        output = io.BytesIO()
        img_format = 'PNG' if img.format == 'JPEG' else img.format
        resized_img.save(output, format=img_format)
        return output.getvalue()






def main():
    st.title("URL to Markdown Converter")
    
    # Streamlit widgets for user inputs
    url = st.text_input("Enter the URL to convert to Markdown:")
                        #value="https://thedieline.com/blog/2024/3/13/knesko-skin-green-jade")
    keyword = st.text_input("Enter the keyword:", value="")
    output_type = st.selectbox("Select the type of file:", options=['markdown', 'pdf'], index=0)
    output_filename = st.text_input("Enter the output Markdown file name:", value="output.md")
    
    # Button to trigger conversion
    convert_button = st.button("Convert URL to Markdown")
    
    if convert_button:
        markdown_text = url_to_markdown(url=url, keyword=keyword, output_file=output_filename)
        
        if markdown_text is not None:
            # Show Markdown text in Streamlit app
            st.text_area("Markdown Text", markdown_text, height=250)
            
            # Save output to a file
            full_filename = f"{output_filename}.{output_type}"
            save_output(markdown_text, full_filename, output_type)
            
            # Create a link for downloading the output file
            with open(full_filename, "rb") as file:
                btn = st.download_button(
                    label="Download file",
                    data=file,
                    file_name=full_filename,
                    mime="text/markdown" if output_type == 'markdown' else "application/pdf"
                )
            if btn:
                st.success("File downloaded successfully!")

if __name__ == "__main__":
    main()





/////////////////// ALL PURPOSE FUNCTIONS /////////////


# Logo Getting Stuff
def find_and_edit_logo(soup, url):
    logo_html = "Logo not found"
    logo_image = None
    for element in soup.find_all(class_=lambda x: x and 'logo' in x.lower()):
        logo_image = element.find('img')
        if logo_image:
            break

    if logo_image and 'src' in logo_image.attrs:
        src = logo_image['src']
        if not src.startswith(('http://', 'https://')):
            src = urljoin(url, src)

        if src.endswith('.svg'):
            try:
                response = requests.get(src)
                response.raise_for_status()
                modified_svg = add_background_to_svg(response.text)
                svg_base64 = base64.b64encode(modified_svg.encode('utf-8')).decode('utf-8')
                logo_html = f'<img src="data:image/svg+xml;base64,{svg_base64}" alt="logo">'
            except Exception as e:
                print(f"Error processing SVG {src}: {e}")
                logo_html = f'<img src="{src}" alt="logo">'
        else:
            try:
                response = requests.get(src)
                img = Image.open(BytesIO(response.content))
                
                if img.mode in ['RGBA', 'P'] and 'transparency' in img.info:
                    is_mostly_white = all(pixel[:3] == (255, 255, 255) for pixel in img.getdata())
                    background_color = 'black' if is_mostly_white else 'white'
                else:
                    background_color = 'white'
                    
                logo_html = f'<img src="{src}" alt="logo on {background_color} background" style="background-color:{background_color}">'
            except Exception as e:
                print(f"Error processing raster image {src}: {e}")
                logo_html = f'<img src="{src}" alt="logo">'

    return logo_html

def add_background_to_svg(svg_content):
    tree = ET.ElementTree(ET.fromstring(svg_content))
    root = tree.getroot()
    ns = {'svg': 'http://www.w3.org/2000/svg'} if 'svg' not in root.tag else {'svg': ''}
    rect = ET.Element('{http://www.w3.org/2000/svg}rect', attrib={'width': '100%', 'height': '100%', 'fill': 'black'})
    root.insert(0, rect)
    return ET.tostring(root, encoding='unicode', method='xml')


# Header Getting Stuff
def get_header(soup):
    h1_tags = soup.find_all('h1')

    h1_texts = [tag.text.strip() for tag in h1_tags]
    article_title = h1_texts[0]

    return f"# {article_title}\n\n"

# Filter Stuff
def filter_paragraphs(soup, keyword, first_paragraph):
    first_paragraph_text = first_paragraph if first_paragraph else None
    paragraphs = soup.find_all('p')

    for paragraph in paragraphs:
        is_different = paragraph.text != first_paragraph_text
        if keyword.lower() not in paragraph.text.lower() and is_different:
            paragraph.decompose()

    return soup

def filter_images(soup):    
    # Get all <p> elements
    paragraphs = soup.find_all('p')
    
    # To handle images before the first <p> and after the last <p>
    first_p = soup.new_tag('p')
    last_p = soup.new_tag('p')
    if paragraphs:
        soup.body.insert(0, first_p)
        soup.body.append(last_p)
    else:
        # If no <p> tags are present, simply return the original content
        return soup
    
    # Refresh the list of paragraphs to include the new dummy tags
    paragraphs = soup.find_all('p')
    
    # Iterate over each pair of consecutive <p> elements
    for i in range(len(paragraphs) - 1):
        current_p = paragraphs[i]
        next_p = paragraphs[i + 1]
        
        # Find all <img> tags between the current <p> and the next <p>
        images = []
        element = current_p.next_sibling
        while element and element != next_p:
            if isinstance(element, Tag) and element.name == 'img':
                images.append(element)
            element = element.next_sibling
        
        # Remove all but the last image in the found list
        for img in images[:-1]:
            img.decompose()

    # Remove the dummy <p> tags
    first_p.decompose()
    last_p.decompose()

    return soup

# First paragraph stuff
def find_paragraph_with_two_sentences_and_image(soup):
    paragraphs = soup.find_all('p')

    for paragraph in paragraphs:
        # Split into sentences and clean them up
        sentences = [sentence.strip() for sentence in paragraph.text.split('.') if sentence.strip()]
        if len(sentences) >= 2:
            # Convert the paragraph element to an independent BeautifulSoup object
            soupy_text = BeautifulSoup(str(paragraph), 'html.parser')  # Create a new soup from the paragraph's HTML string
            
            # Find the first preceding image
            image = None
            previous_elements = paragraph.find_all_previous('img')
            if previous_elements:
                image = previous_elements[0].get('src')  # Getting the src attribute of the first preceding image

            # Return the new BeautifulSoup object of the paragraph and the image source
            return soupy_text, image

    # Return None if no suitable paragraph is found
    return None, None


# Main Function
def url_to_html(url, keyword='', output_file='output'):
    try:
        # Fetch the content from the URL
        response = requests.get(url)
        response.raise_for_status()  # Will raise an exception for bad responses
    except requests.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return None

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    logo_html = find_and_edit_logo(soup, url)
    article_title = get_header(soup)
    first_paragraph, first_image = find_paragraph_with_two_sentences_and_image(soup)
    first_image_html = f'<img src="{first_image}" style="display: block; margin: auto; height: 100px; width: auto;">' if first_image else ""

    soup = filter_paragraphs(soup, keyword, first_paragraph)
    soup = filter_images(soup)

    # Clean up HTML
    for element in soup.find_all(['nav', 'header', 'footer', 'title']):
        element.decompose()
    for element in soup.find_all(class_=["nav", "menu", "header", "footer"]):
        element.decompose()

    html_content = logo_html + article_title + first_image_html + str(first_paragraph) + str(soup)

def process_image_logo(src):
    try:
        response = requests.get(src)
        response.raise_for_status()
        img = Image.open(BytesIO(response.content))
        
        if img.mode in ['RGBA', 'P'] and 'transparency' in img.info:
            is_mostly_white = all(pixel[:3] == (255, 255, 255) for pixel in img.getdata())
            background_color = 'black' if is_mostly_white else 'white'
        else:
            background_color = 'white'
        
        new_tag = soup.new_tag("img", src=src, alt=f"logo on {background_color} background")
    except Exception as e:
        print(f"Error processing raster image {src}: {e}")
        new_tag = soup.new_tag("img", src=src)
    
    return new_tag

def add_background_to_svg(svg_content):
    tree = ET.ElementTree(ET.fromstring(svg_content))
    root = tree.getroot()
    rect = ET.Element('{http://www.w3.org/2000/svg}rect', attrib={'width': '100%', 'height': '100%', 'fill': 'black'})
    root.insert(0, rect)
    return ET.tostring(root, encoding='unicode', method='xml')



def find_logo(soup, site):
    pos = site.find('.com')
    if pos != -1:
        main_site = site[:pos+4]
    else:
        main_site = site

    logo_elements = soup.find_all(class_=lambda x: x and 'logo' in x.lower())
    print(main_site)

    for element in logo_elements:
        parent_link = element.find_parent('a')
        if parent_link and parent_link.get('href', '').endswith(main_site):
            if element.name == 'img':
                return element
            elif element.name == 'svg':
                return element
            else:
                nested_img = element.find('img')
                if nested_img:
                    return nested_img
                nested_svg = element.find('svg')
                if nested_svg:
                    return nested_svg

    return None


def primary_func(url, keyword, output_type, output_name):
    soup = get_webpage_info(url)

    logo = find_logo(soup, url)
    article_title = get_header(soup)
    first_paragraph, first_image = find_paragraph_with_two_sentences_and_image(soup)

    soup = filter_paragraphs(soup=soup, keyword=keyword, first_paragraph=first_paragraph)
    soup = filter_images(soup)

    clean_html = str('')
    if logo:
        with open('logo_debug.html', 'w') as debug_file:
            debug_file.write(str(logo))
        print("Logo HTML written to logo_debug.html")
        clean_html += str(logo)

    if article_title:
        clean_html += str(article_title)
    if first_image:
        clean_html += str(first_image)
    if first_paragraph:
        clean_html += str(first_paragraph)

    # Clean up HTML
    for element in soup.find_all(['nav', 'header', 'footer', 'title']):
        element.decompose()
    for element in soup.find_all(class_=["nav", "menu", "header", "footer"]):
        element.decompose()
    #for script in soup(["script", "style"]):
    #    script.decompose()

    clean_html += str(soup)
    save_document_to_pdf(clean_html=clean_html, output_name=output_name)

    return soup

    from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException, ElementNotVisibleException, NoSuchElementException, ElementNotSelectableException
from weasyprint import HTML
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup, Tag
import requests
from PIL import Image
from io import BytesIO
import base64
import re
import xml.etree.ElementTree as ET

# Get desirable page elements
def find_logo(soup, link):
    pos = link.find('.com')
    if pos != -1:
        main_site = link[:pos+4]
    else:
        main_site = link

    logo_pattern = re.compile(r'\blogo\b', re.IGNORECASE)
    logo_elements = soup.find_all(class_=logo_pattern)
    updated_elements = []

    for element in logo_elements:
        if element.name == 'img':
            new_link = soup.new_tag('a', href=main_site)
            element.insert_before(new_link)
            new_link.append(element)
            updated_elements.append(new_link)
            # Apply size restrictions to img
            element['style'] = 'max-width:100px; max-height:100px;'
        elif element.name == 'svg':
            updated_elements.append(element)
            # Apply size restrictions to SVG
            element['width'] = '100'
            element['height'] = '100'
            # Optional: set style for max-width and max-height
            element['style'] = 'max-width:100px; max-height:100px;'
        elif element.name == 'div':
            new_link = soup.new_tag('a', href=main_site)
            element.wrap(new_link)
            updated_elements.append(new_link)

    if updated_elements:
        return updated_elements[0]
    else:
        return None

def get_header(soup):
    h1_tag = soup.find('h1')
    return h1_tag if h1_tag else None

# Find Core Div Box
def find_lowest_common_ancestor(tag1, tag2):
    """Finds the lowest common ancestor of two tags."""
    ancestors_tag1 = set(tag1.parents)
    for ancestor in tag2.parents:
        if ancestor in ancestors_tag1:
            return ancestor

def find_paragraphs_with_two_sentences_and_image(soup):
    paragraphs = soup.find_all('p')
    first_paragraph = None
    last_paragraph = None

    for paragraph in paragraphs:
        sentences = [sentence.strip() for sentence in paragraph.text.split('.') if sentence.strip()]
        if len(sentences) >= 2:
            image = None
            previous_elements = paragraph.find_all_previous()
            for elem in previous_elements:
                if elem.name == 'img':
                    image = elem
                    break

            if image:
                if not first_paragraph:
                    first_paragraph = (paragraph, image)
                last_paragraph = (paragraph, image)

    return first_paragraph, last_paragraph

def find_common_container(soup):
    first, last = find_paragraphs_with_two_sentences_and_image(soup)
    if first and last:
        # Extract the paragraph elements from the tuples
        first_paragraph, _ = first
        last_paragraph, _ = last
        # Find the lowest common ancestor div
        lca = find_lowest_common_ancestor(first_paragraph, last_paragraph)
        while lca and lca.name != 'div':
            lca = lca.parent
        return lca
    return None

def decompose_outside_lca(soup, lca):
    current = lca
    path = set()
    
    # Build the set of all ancestors of the LCA including itself
    while current is not None:
        path.add(current)
        current = current.parent

    # Decompose all top-level elements not in the path
    for child in soup.recursiveChildGenerator():
        if isinstance(child, Tag) and child not in path:
            child.decompose()

# Apply filters
def filter_paragraphs(soup, keyword, first_paragraph):
    first_paragraph_text = first_paragraph.text if first_paragraph else None
    paragraphs = soup.find_all('p')

    for paragraph in paragraphs:
        is_different = paragraph.text != first_paragraph_text
        if keyword.lower() not in paragraph.text.lower() and is_different:
            paragraph.decompose()

    return soup

def filter_images(soup):
    paragraphs = soup.find_all('p')
    
    first_p = soup.new_tag('p')
    last_p = soup.new_tag('p')
    if paragraphs:
        soup.body.insert(0, first_p)
        soup.body.append(last_p)
    else:
        return soup
    
    # Flag to start removing images after last_p is encountered
    remove_images = False
    element = soup.body.find()  # Start from the first element in the body

    while element:
        if element == last_p:
            remove_images = True  # Set flag to start removing images

        if remove_images and isinstance(element, Tag) and element.name == 'img':
            element.decompose()  # Remove the image

        element = element.next_sibling  # Move to the next element

    # Remove the dummy <p> tags
    first_p.decompose()
    last_p.decompose()

    return soup

# Get webpage using selenium
def get_webpage_info(url):
    options = Options()
    prefs = {
        "download.default_directory": "./",
        "credentials_enable_service": False,
        "profile.password_manager_enabled": False
    }
    options.add_experimental_option("prefs", prefs)
    options.add_argument('--disable-popup-blocking')  # Explicitly allow popups to manage them
    options.add_argument('--headless')  # Run in headless mode (no UI)
    options.add_argument('--no-sandbox')  # For certain environments like CI
    options.add_argument('--disable-dev-shm-usage')  # Helps with resource issues
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument("--disable-extensions")
    options.add_experimental_option('useAutomationExtension', False)
    options.add_experimental_option("excludeSwitches", ["enable-automation"])

    #service = Service(executable_path='/usr/bin/chromedriver')
    driver = webdriver.Chrome(options=options)
    driver.get("https://www.google.com/")
    chrome_driver = webdriver.Chrome(options=options)

    chrome_driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    try:
        driver.get(url)
        wait = WebDriverWait(driver, 5)  # Increase timeout if necessary

        # Attempt to close any popups
        try:
            popup_selector = "div[class^='popupClass']"
            close_button_selector = "button[class^='closeButtonClass']"
            popup = wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, popup_selector)))
            close_button = driver.find_element(By.CSS_SELECTOR, close_button_selector)
            close_button.click()
        except TimeoutException:
            print("No popup appeared within the timeout period.")
        except ElementClickInterceptedException:
            print("Popup close button was not clickable.")

        # Wait for specific elements to load
        try:
            elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div[class^='someclass']")))
        except TimeoutException:
            print("Specified elements did not appear within the timeout period.")
            elements = []  # Assign an empty list if elements are not found

        html_content = driver.page_source
    finally:
        driver.quit()

    soup = BeautifulSoup(html_content, 'html.parser')
    return soup


# Save webpage using weasyprint
def save_document_to_pdf(clean_html, output_name):
    HTML(string=clean_html).write_pdf(f'{output_name}.pdf')
    print("PDF has been created successfully!")

def insert_logo(soup, logo_content):
    print(logo_content)
    # Check the type of logo_content
    if isinstance(logo_content, Tag):
        # If logo_content is a BeautifulSoup Tag object
        logo_tag = logo_content
    elif logo_content.strip().lower().endswith('.svg'):
        # Assume logo_content is a URL to an SVG image
        logo_tag = soup.new_tag('img', src=logo_content)
        logo_tag['alt'] = 'Company Logo'
    elif '<svg' in logo_content:
        # Raw SVG content
        logo_tag = BeautifulSoup(logo_content, 'html.parser').find('svg')
    else:
        # Typical image URL
        logo_tag = soup.new_tag('img', src=logo_content)
        logo_tag['alt'] = 'Company Logo'

    # Add style to constrain the logo dimensions if it's an 'img' tag
    if logo_tag.name == 'img':
        logo_tag['style'] = 'max-width:100px; max-height:100px;'

    # Find or create the header to insert the logo
    header = soup.find('header')
    if not header:
        header = soup.new_tag('header')
        soup.body.insert(0, header)

    # Ensure the logo_tag is not already part of another structure
    if logo_tag.parent:
        logo_tag = logo_tag.extract()

    # Insert the logo at the beginning of the header
    header.insert(0, logo_tag)

def primary_func(url, keyword, output_type, output_name):
    soup = get_webpage_info(url)

    logo = find_logo(soup, url)

    # Insert logo
    insert_logo(soup, logo)

    article_title = get_header(soup)
    first_paragraph, first_image = find_paragraph_with_two_sentences_and_image(soup)

    soup = filter_paragraphs(soup=soup, keyword=keyword, first_paragraph=first_paragraph)
    soup = filter_images(soup)

    # Clean up HTML
    for element in soup.find_all(['nav', 'footer', 'title']):
        element.decompose()
    for element in soup.find_all(class_=["nav", "menu", "header", "footer"]):
        element.decompose()

    clean_html = str(soup)
    with open('doc_debug.html', 'w') as debug_file:
        debug_file.write(str(clean_html))

    save_document_to_pdf(clean_html=clean_html, output_name=output_name)

# Implementation (change this for stremamline)
if __name__ == "__main__":
    url = input("Enter the URL to clip: ")
    url = url if url else "https://thedieline.com/blog/2024/3/13/knesko-skin-green-jade"
    keyword = input("Enter the keyword: ")
    keyword = keyword if keyword else ''
    output_type = input("Enter the type of file: ")
    output_type = output_type if output_type else 'pdf'
    output_filename = input("Enter the output file name (default 'output.pdf'): ")
    output_filename = output_filename if output_filename else 'output'
    primary_func(url=url, keyword=keyword, output_name=output_filename, output_type=output_type)

if __name__ == "__main__":
    url = input("Enter the URL to clip: ")
    url = url if url else "https://thedieline.com/blog/2024/3/13/knesko-skin-green-jade"
    keyword = input("Enter the keyword: ")
    keyword = keyword if keyword else ''
    output_type = input("Enter the type of file: ")
    output_type = output_type if output_type else 'pdf'
    output_filename = input("Enter the output file name (default 'output.pdf'): ")
    output_filename = output_filename if output_filename else 'output'
    primary_func(url=url, keyword=keyword, output_name=output_filename, output_type=output_type)
