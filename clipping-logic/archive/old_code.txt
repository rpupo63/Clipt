
def resize_image_to_same_height(image_data, target_height=300):
    # Try to detect if the image is an SVG by looking at the first few bytes
    if image_data[:5].lower() == b'<?xml' or b'<svg' in image_data[:50].lower():
        # Parse the SVG data
        tree = ET.ElementTree(ET.fromstring(image_data))
        svg = tree.getroot()
        # Check for viewBox attribute
        viewBox = svg.get('viewBox')
        if viewBox:
            _, _, vb_width, vb_height = map(float, viewBox.split())
        else:
            vb_width = vb_height = None

        width = svg.get('width')
        height = svg.get('height')
        
        if width and height:
            original_width = float(width.strip('px'))
            original_height = float(height.strip('px'))
        elif vb_width and vb_height:
            original_width, original_height = vb_width, vb_height
        else:
            raise ValueError("SVG does not have sufficient information for resizing (no width/height or viewBox)")

        # Calculate new dimensions
        scale_ratio = target_height / original_height
        new_width = int(original_width * scale_ratio)

        # Set new width and height
        svg.set('width', f"{new_width}px")
        svg.set('height', f"{target_height}px")

        # Convert tree back to bytes
        output = io.BytesIO()
        tree.write(output, encoding='utf-8', xml_declaration=True)
        return output.getvalue()
    else:
        # Handle raster images
        img = Image.open(io.BytesIO(image_data))
        original_width, original_height = img.size
        scale_ratio = target_height / original_height
        new_width = int(original_width * scale_ratio)
        resized_img = img.resize((new_width, target_height), Image.Resampling.LANCZOS)
        output = io.BytesIO()
        img_format = 'PNG' if img.format == 'JPEG' else img.format
        resized_img.save(output, format=img_format)
        return output.getvalue()






def main():
    st.title("URL to Markdown Converter")
    
    # Streamlit widgets for user inputs
    url = st.text_input("Enter the URL to convert to Markdown:")
                        #value="https://thedieline.com/blog/2024/3/13/knesko-skin-green-jade")
    keyword = st.text_input("Enter the keyword:", value="")
    output_type = st.selectbox("Select the type of file:", options=['markdown', 'pdf'], index=0)
    output_filename = st.text_input("Enter the output Markdown file name:", value="output.md")
    
    # Button to trigger conversion
    convert_button = st.button("Convert URL to Markdown")
    
    if convert_button:
        markdown_text = url_to_markdown(url=url, keyword=keyword, output_file=output_filename)
        
        if markdown_text is not None:
            # Show Markdown text in Streamlit app
            st.text_area("Markdown Text", markdown_text, height=250)
            
            # Save output to a file
            full_filename = f"{output_filename}.{output_type}"
            save_output(markdown_text, full_filename, output_type)
            
            # Create a link for downloading the output file
            with open(full_filename, "rb") as file:
                btn = st.download_button(
                    label="Download file",
                    data=file,
                    file_name=full_filename,
                    mime="text/markdown" if output_type == 'markdown' else "application/pdf"
                )
            if btn:
                st.success("File downloaded successfully!")

if __name__ == "__main__":
    main()





/////////////////// ALL PURPOSE FUNCTIONS /////////////


# Logo Getting Stuff
def find_and_edit_logo(soup, url):
    logo_html = "Logo not found"
    logo_image = None
    for element in soup.find_all(class_=lambda x: x and 'logo' in x.lower()):
        logo_image = element.find('img')
        if logo_image:
            break

    if logo_image and 'src' in logo_image.attrs:
        src = logo_image['src']
        if not src.startswith(('http://', 'https://')):
            src = urljoin(url, src)

        if src.endswith('.svg'):
            try:
                response = requests.get(src)
                response.raise_for_status()
                modified_svg = add_background_to_svg(response.text)
                svg_base64 = base64.b64encode(modified_svg.encode('utf-8')).decode('utf-8')
                logo_html = f'<img src="data:image/svg+xml;base64,{svg_base64}" alt="logo">'
            except Exception as e:
                print(f"Error processing SVG {src}: {e}")
                logo_html = f'<img src="{src}" alt="logo">'
        else:
            try:
                response = requests.get(src)
                img = Image.open(BytesIO(response.content))
                
                if img.mode in ['RGBA', 'P'] and 'transparency' in img.info:
                    is_mostly_white = all(pixel[:3] == (255, 255, 255) for pixel in img.getdata())
                    background_color = 'black' if is_mostly_white else 'white'
                else:
                    background_color = 'white'
                    
                logo_html = f'<img src="{src}" alt="logo on {background_color} background" style="background-color:{background_color}">'
            except Exception as e:
                print(f"Error processing raster image {src}: {e}")
                logo_html = f'<img src="{src}" alt="logo">'

    return logo_html

def add_background_to_svg(svg_content):
    tree = ET.ElementTree(ET.fromstring(svg_content))
    root = tree.getroot()
    ns = {'svg': 'http://www.w3.org/2000/svg'} if 'svg' not in root.tag else {'svg': ''}
    rect = ET.Element('{http://www.w3.org/2000/svg}rect', attrib={'width': '100%', 'height': '100%', 'fill': 'black'})
    root.insert(0, rect)
    return ET.tostring(root, encoding='unicode', method='xml')


# Header Getting Stuff
def get_header(soup):
    h1_tags = soup.find_all('h1')

    h1_texts = [tag.text.strip() for tag in h1_tags]
    article_title = h1_texts[0]

    return f"# {article_title}\n\n"

# Filter Stuff
def filter_paragraphs(soup, keyword, first_paragraph):
    first_paragraph_text = first_paragraph if first_paragraph else None
    paragraphs = soup.find_all('p')

    for paragraph in paragraphs:
        is_different = paragraph.text != first_paragraph_text
        if keyword.lower() not in paragraph.text.lower() and is_different:
            paragraph.decompose()

    return soup

def filter_images(soup):    
    # Get all <p> elements
    paragraphs = soup.find_all('p')
    
    # To handle images before the first <p> and after the last <p>
    first_p = soup.new_tag('p')
    last_p = soup.new_tag('p')
    if paragraphs:
        soup.body.insert(0, first_p)
        soup.body.append(last_p)
    else:
        # If no <p> tags are present, simply return the original content
        return soup
    
    # Refresh the list of paragraphs to include the new dummy tags
    paragraphs = soup.find_all('p')
    
    # Iterate over each pair of consecutive <p> elements
    for i in range(len(paragraphs) - 1):
        current_p = paragraphs[i]
        next_p = paragraphs[i + 1]
        
        # Find all <img> tags between the current <p> and the next <p>
        images = []
        element = current_p.next_sibling
        while element and element != next_p:
            if isinstance(element, Tag) and element.name == 'img':
                images.append(element)
            element = element.next_sibling
        
        # Remove all but the last image in the found list
        for img in images[:-1]:
            img.decompose()

    # Remove the dummy <p> tags
    first_p.decompose()
    last_p.decompose()

    return soup

# First paragraph stuff
def find_paragraph_with_two_sentences_and_image(soup):
    paragraphs = soup.find_all('p')

    for paragraph in paragraphs:
        # Split into sentences and clean them up
        sentences = [sentence.strip() for sentence in paragraph.text.split('.') if sentence.strip()]
        if len(sentences) >= 2:
            # Convert the paragraph element to an independent BeautifulSoup object
            soupy_text = BeautifulSoup(str(paragraph), 'html.parser')  # Create a new soup from the paragraph's HTML string
            
            # Find the first preceding image
            image = None
            previous_elements = paragraph.find_all_previous('img')
            if previous_elements:
                image = previous_elements[0].get('src')  # Getting the src attribute of the first preceding image

            # Return the new BeautifulSoup object of the paragraph and the image source
            return soupy_text, image

    # Return None if no suitable paragraph is found
    return None, None


# Main Function
def url_to_html(url, keyword='', output_file='output'):
    try:
        # Fetch the content from the URL
        response = requests.get(url)
        response.raise_for_status()  # Will raise an exception for bad responses
    except requests.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return None

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    logo_html = find_and_edit_logo(soup, url)
    article_title = get_header(soup)
    first_paragraph, first_image = find_paragraph_with_two_sentences_and_image(soup)
    first_image_html = f'<img src="{first_image}" style="display: block; margin: auto; height: 100px; width: auto;">' if first_image else ""

    soup = filter_paragraphs(soup, keyword, first_paragraph)
    soup = filter_images(soup)

    # Clean up HTML
    for element in soup.find_all(['nav', 'header', 'footer', 'title']):
        element.decompose()
    for element in soup.find_all(class_=["nav", "menu", "header", "footer"]):
        element.decompose()

    html_content = logo_html + article_title + first_image_html + str(first_paragraph) + str(soup)